{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d95bea7-b0b3-4b94-9cfa-631ca7686ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter web url or enter:  https://cpdaccredited.wixsite.com/cld-institute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.dr-chuck.com', 'https://www.duepoint.net', 'https://generationalwealthafrica.com', 'https://cpdaccredited.wixsite.com/cld-institute']\n",
      "29 https://cpdaccredited.wixsite.com/cld-institute (610705) 10\n",
      "35 https://cpdaccredited.wixsite.com/cld-institute/news-resources-1 (631779) 10\n",
      "30 https://cpdaccredited.wixsite.com/cld-institute/about (636488) 10\n",
      "33 https://cpdaccredited.wixsite.com/cld-institute/blank-page-2 (625103) 10\n",
      "36 https://cpdaccredited.wixsite.com/cld-institute/contact-us (650711) 10\n",
      "31 https://cpdaccredited.wixsite.com/cld-institute/courses (881498) 10\n",
      "34 https://cpdaccredited.wixsite.com/cld-institute/blank-page-1-1 (639465) 10\n",
      "32 https://cpdaccredited.wixsite.com/cld-institute/publications-1 (672237) 10\n",
      "No unretrieved HTML pages found\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import urllib.error\n",
    "import ssl\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Connect to SQLite database or create one if it doesn't exist\n",
    "conn = sqlite3.connect('spider.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create table for storing web pages with their HTML and ranking data\n",
    "cur.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS Pages\n",
    "            (id INTEGER PRIMARY KEY, \n",
    "            url TEXT UNIQUE, \n",
    "            html TEXT,\n",
    "            error INTEGER, old_rank REAL, new_rank REAL)\n",
    "            \n",
    "            ''')\n",
    "\n",
    "# Create table for storing links between pages (edges in the graph)\n",
    "cur.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS Links\n",
    "            (from_id INTEGER, \n",
    "            to_id INTEGER,\n",
    "            UNIQUE(from_id, to_id))\n",
    "            \n",
    "            ''')\n",
    "\n",
    "# Create table to track domains (websites) being crawled\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n",
    "\n",
    "# Check if there is an unfinished crawl by looking for pages without HTML or errors\n",
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print(\"Restarting existing crawl. Remove spider.sqlite to start a fresh crawl.\")\n",
    "else:\n",
    "    # Input starting URL if no existing crawl\n",
    "    starturl = input('Enter web url or enter: ')\n",
    "    if len(starturl) < 1: \n",
    "        starturl = 'http://www.dr-chuck.com/'  # Default URL\n",
    "    if starturl.endswith('/'):\n",
    "        starturl = starturl[:-1]\n",
    "    \n",
    "    web = starturl\n",
    "    # Strip .htm or .html from URL if present to get the base domain\n",
    "    if starturl.endswith('.htm') or starturl.endswith('.html'):\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    # Insert the starting website into the Webs and Pages tables\n",
    "    if len(web) > 1:\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', (web,))\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', (starturl,))\n",
    "        conn.commit()\n",
    "\n",
    "# Retrieve the list of all tracked websites (domains) in the database\n",
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print(webs)\n",
    "\n",
    "# Start crawling process, with a limit of 100 pages\n",
    "many = 100  # Number of pages to retrieve\n",
    "while True:\n",
    "    if many < 1:\n",
    "        sval = input('How many pages:')\n",
    "        if len(sval) < 1: \n",
    "            break\n",
    "        many = int(sval)\n",
    "    many = many - 1\n",
    "\n",
    "    # Fetch the next page to crawl (which has not been crawled yet)\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print('No unretrieved HTML pages found')\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print(fromid, url, end=' ')\n",
    "\n",
    "    # Delete all existing links from the page before retrieving it again\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid,))\n",
    "    try:\n",
    "        # Open the URL and read its content\n",
    "        document = urlopen(url, context=ctx)\n",
    "        html = document.read()\n",
    "\n",
    "        # Check for HTTP error codes and handle non-HTML content\n",
    "        if document.getcode() != 200:\n",
    "            print(\"Error on page:\", document.getcode())\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url))\n",
    "\n",
    "        if 'text/html' != document.info().get_content_type():\n",
    "            print(\"Ignore non text/html page\")\n",
    "            cur.execute('DELETE FROM Pages WHERE url=?', (url,))\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print('(' + str(len(html)) + ')', end=' ')\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('Program interrupted by user...')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Unable to retrieve or parse page\")\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url,))\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    # Store the HTML content in the database\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', (url,))\n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url))\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all anchor tags (links) from the page\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if href is None:\n",
    "            continue\n",
    "\n",
    "        # Resolve relative URLs to absolute URLs\n",
    "        up = urlparse(href)\n",
    "        if len(up.scheme) < 1:\n",
    "            href = urljoin(url, href)\n",
    "\n",
    "        # Ignore fragment identifiers (anything after #)\n",
    "        ipos = href.find('#')\n",
    "        if ipos > 1:\n",
    "            href = href[:ipos]\n",
    "\n",
    "        # Skip non-HTML files (images, etc.) and trailing slashes\n",
    "        if href.endswith(('.png', '.jpg', '.gif')): \n",
    "            continue\n",
    "        if href.endswith('/'):\n",
    "            href = href[:-1]\n",
    "\n",
    "        if len(href) < 1:\n",
    "            continue\n",
    "\n",
    "        # Check if the URL belongs to any of the tracked websites (webs)\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if href.startswith(web):\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            continue\n",
    "\n",
    "        # Insert the new page URL into the Pages table (if not already present)\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', (href,))\n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        # Retrieve the ID of the inserted page and create a link from the current page\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', (href,))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print('Could not retrieve id')\n",
    "            continue\n",
    "\n",
    "        # Insert the link between the current page and the linked page\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', (fromid, toid))\n",
    "\n",
    "    print(count)\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33089a4-abaa-4156-8d8b-34becbf6086f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
